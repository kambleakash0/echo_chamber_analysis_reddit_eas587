{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40ab5e7-a133-4ee7-888c-cd0cdd1a0b18",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd5cc80-9fb3-429c-8990-b9f0ee8e3421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "âœ… Setup Complete (Intermediate Power Mode).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
    "\n",
    "# Spark MLlib Imports\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Initialize Spark Session with \"Intermediate\" settings\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = \"data_cleaned\"\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "if os.path.exists(MODELS_DIR):\n",
    "    shutil.rmtree(MODELS_DIR)\n",
    "os.makedirs(MODELS_DIR)\n",
    "\n",
    "print(\"âœ… Setup Complete (Intermediate Power Mode).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737c19d-8398-4a20-b30d-50f3a0e936fd",
   "metadata": {},
   "source": [
    "### Robust Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e7e464-de9f-469a-affd-c13cb8f2e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records for ML (Cleaned): 32,233\n",
      "+-----------------------------------------------------------------------------------------------------+---------+\n",
      "|title                                                                                                |subreddit|\n",
      "+-----------------------------------------------------------------------------------------------------+---------+\n",
      "|Parishioners hold prayer at Naval Station Great Lakes ahead of planned immigration crackdown         |politics |\n",
      "|What challenges are their in your mind leaning on capitalism, socialism, and communism?              |politics |\n",
      "|RFK Jr, who doesnâ€™t have a medical degree, says he can diagnose children just by just looking at them|politics |\n",
      "|'Something is very wrong': Ex-Trump insider flags 'alarming' news about president's health           |politics |\n",
      "|Florida Democrat: DeSantis, Trump â€˜declared war on people of colorâ€™                                  |politics |\n",
      "+-----------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all posts data\n",
    "posts_path = f\"{DATA_DIR}/*_posts.csv\"\n",
    "\n",
    "# 1. Read as strings\n",
    "df_raw = spark.read.csv(posts_path, header=True, inferSchema=False)\n",
    "\n",
    "# 2. Define Valid Subreddits (The Allow-List)\n",
    "valid_subreddits = [\n",
    "    'Conservative', 'Libertarian', 'PoliticalDiscussion', \n",
    "    'neutralnews', 'politics', 'socialism', 'worldnews'\n",
    "]\n",
    "\n",
    "# 3. Robust cleaning and FILTERING\n",
    "df_clean = df_raw.withColumn(\n",
    "    \"timestamp\", F.to_timestamp(F.from_unixtime(F.expr(\"try_cast(created_utc AS LONG)\")))\n",
    ").withColumn(\n",
    "    \"score\", F.expr(\"try_cast(score AS INT)\")\n",
    ").withColumn(\n",
    "    \"num_comments\", F.expr(\"try_cast(num_comments AS INT)\")\n",
    ").filter(\n",
    "    F.col(\"subreddit\").isin(valid_subreddits)  # <--- CRITICAL FIX: Drop rows with garbage subreddits\n",
    ").dropna(subset=[\"title\", \"subreddit\"])\n",
    "\n",
    "# 4. Select only what we need for ML\n",
    "df_ml = df_clean.select(\"title\", \"subreddit\")\n",
    "\n",
    "print(f\"Total records for ML (Cleaned): {df_ml.count():,}\")\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9688915-f71e-4fed-867b-38893ccdb305",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df65406-5a6f-4846-8bc9-0420bc5a0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:\n",
      "  politics: 0.3587\n",
      "  Conservative: 0.4147\n",
      "  worldnews: 0.8576\n",
      "  socialism: 3.9593\n",
      "  PoliticalDiscussion: 4.5456\n",
      "  Libertarian: 7.7914\n",
      "  neutralnews: 29.9007\n",
      "Training Set: 25,853 | Test Set: 6,380\n"
     ]
    }
   ],
   "source": [
    "# Spark MLlib doesn't handle class imbalance automatically like sklearn's class_weight='balanced'.\n",
    "# We must calculate weights manually and add them as a column.\n",
    "\n",
    "# 1. Count totals per class\n",
    "class_counts = df_ml.groupBy(\"subreddit\").count().collect()\n",
    "total_count = df_ml.count()\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# 2. Calculate weights: Weight = Total / (NumClasses * ClassCount)\n",
    "# This gives higher weight to minority classes\n",
    "balancing_ratios = {}\n",
    "for row in class_counts:\n",
    "    balancing_ratios[row['subreddit']] = total_count / (num_classes * row['count'])\n",
    "\n",
    "print(\"Class Weights:\")\n",
    "for sub, weight in balancing_ratios.items():\n",
    "    print(f\"  {sub}: {weight:.4f}\")\n",
    "\n",
    "# 3. Create a function to map subreddit to weight\n",
    "def get_weight(subreddit):\n",
    "    return float(balancing_ratios.get(subreddit, 1.0))\n",
    "\n",
    "# Register UDF\n",
    "weight_udf = F.udf(get_weight, DoubleType())\n",
    "\n",
    "# 4. Add weight column\n",
    "from pyspark.sql.types import DoubleType\n",
    "df_weighted = df_ml.withColumn(\"classWeight\", weight_udf(F.col(\"subreddit\")))\n",
    "\n",
    "# 5. Split into Train/Test\n",
    "train_data, test_data = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training Set: {train_data.count():,} | Test Set: {test_data.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63aa7eb-1c71-4997-83ac-13ba4341bd4f",
   "metadata": {},
   "source": [
    "### Building the Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8923b283-b7ba-4c8e-a1bb-bd7bdea0786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature Engineering stages defined.\n"
     ]
    }
   ],
   "source": [
    "# We define the stages that transform text into numbers.\n",
    "# These stages will be shared across our supervised models.\n",
    "\n",
    "# 1. Convert target string (subreddit) to index (0, 1, 2...)\n",
    "label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\").fit(train_data)\n",
    "\n",
    "# 2. Tokenize: Split sentences into words\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "\n",
    "# 3. Remove Stop Words: Remove \"the\", \"a\", \"is\", etc.\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# 4. TF (HashingTF): Convert words to feature vectors (raw counts)\n",
    "# numFeatures=10000 matches our Phase 2 logic\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# 5. IDF: Rescale features based on how rare they are across documents\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 6. Convert prediction index back to string (for human readable output)\n",
    "# We need the labels from the indexer\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_subreddit\", labels=label_indexer.labels)\n",
    "\n",
    "print(\"âœ… Feature Engineering stages defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a05cd1-7a9d-412e-a1c0-052b802e5486",
   "metadata": {},
   "source": [
    "### Additional Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07dc8da-2ac9-457a-bad6-ebce4500b6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string, subreddit: string, classWeight: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Reduce Spark memory overhead\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")  # Default is 200\n",
    "# spark.conf.set(\"spark.default.parallelism\", \"4\")\n",
    "\n",
    "# # Unpersist cached data if you cached train_data earlier\n",
    "# train_data.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdc982-68a7-45dc-8b7f-d4fd3a334e4b",
   "metadata": {},
   "source": [
    "### Model 1 - Logistic Regression (with Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c21b71-af9a-4d9b-90fa-bd4747fc75a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1: Logistic Regression ---\n",
      "Training with Cross-Validation (this may take a minute)...\n",
      "âœ… Logistic Regression Accuracy: 0.5671\n",
      "ðŸ’¾ Model saved to: models/spark_lr_model\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Model 1: Logistic Regression ---\")\n",
    "\n",
    "# 1. Define the Classifier\n",
    "# Note: We pass the 'classWeight' column we created earlier\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"classWeight\")\n",
    "\n",
    "# 2. Build the Pipeline\n",
    "lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, hashingTF, idf, lr, label_converter])\n",
    "\n",
    "# 3. Define Hyperparameter Grid for Cross-Validation\n",
    "# We will tune the regularization parameter (regParam)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "# 4. Set up CrossValidator (3-fold)\n",
    "crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# 5. Train (Fit)\n",
    "print(\"Training with Cross-Validation (this may take a minute)...\")\n",
    "cv_model_lr = crossval.fit(train_data)\n",
    "best_lr_model = cv_model_lr.bestModel\n",
    "\n",
    "# 6. Evaluate\n",
    "predictions_lr = best_lr_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "print(f\"âœ… Logistic Regression Accuracy: {accuracy_lr:.4f}\")\n",
    "\n",
    "# 7. Save the Model (Crucial for MCP)\n",
    "model_path = f\"{MODELS_DIR}/spark_lr_model\"\n",
    "best_lr_model.save(model_path)\n",
    "print(f\"ðŸ’¾ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776aaa1-f0ad-4d4c-a97e-4552114af3f8",
   "metadata": {},
   "source": [
    "### Model 2 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310662a2-5249-4ebd-a836-b32a8b19c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 2: Naive Bayes ---\n",
      "âœ… Naive Bayes Accuracy: 0.5464\n",
      "ðŸ’¾ Model saved to: models/spark_nb_model\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Model 2: Naive Bayes ---\")\n",
    "\n",
    "# 1. Define Classifier\n",
    "# Naive Bayes in Spark does not support weightCol, but is naturally robust.\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# 2. Build Pipeline\n",
    "nb_pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, hashingTF, idf, nb, label_converter])\n",
    "\n",
    "# 3. Train\n",
    "nb_model = nb_pipeline.fit(train_data)\n",
    "\n",
    "# 4. Evaluate\n",
    "predictions_nb = nb_model.transform(test_data)\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "print(f\"âœ… Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n",
    "\n",
    "# 5. Save Model\n",
    "model_path = f\"{MODELS_DIR}/spark_nb_model\"\n",
    "nb_model.save(model_path)\n",
    "print(f\"ðŸ’¾ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84516ce4-b241-4a7c-8626-45e114e8c6a5",
   "metadata": {},
   "source": [
    "### Model 3 - K-Means Clustering (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3556125-320c-4483-bf3c-8da583777cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 3: K-Means Clustering (In-Memory) ---\n",
      "Materializing training data into RAM... Count: 22,696\n",
      "Training K-Means...\n",
      "âœ… Training Complete.\n",
      "âœ… K-Means Silhouette Score: 0.0141\n",
      "ðŸ’¾ Model saved to: models/spark_kmeans_model\n",
      "ðŸ§¹ RAM cleared.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Model 3: K-Means Clustering (In-Memory) ---\")\n",
    "\n",
    "# 1. Split Data\n",
    "train_set, val_set, test_set = df_weighted.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# --- CRITICAL STEP: Cache and Force Materialization ---\n",
    "# We cache the training set to RAM.\n",
    "# We MUST call .count() immediately to force Spark to compute the split \n",
    "# and store it in memory right now. This prevents the \"zip RDD\" error.\n",
    "train_set.cache()\n",
    "print(f\"Materializing training data into RAM... Count: {train_set.count():,}\")\n",
    "\n",
    "# 2. Define K-Means\n",
    "# k=7 (for 7 subreddits)\n",
    "kmeans = KMeans(featuresCol=\"features\", k=7, seed=42)\n",
    "\n",
    "# 3. Build Pipeline\n",
    "# Note: We don't need label_indexer or label_converter for unsupervised learning\n",
    "kmeans_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, kmeans])\n",
    "\n",
    "# 4. Train\n",
    "print(\"Training K-Means...\")\n",
    "kmeans_model = kmeans_pipeline.fit(train_set)\n",
    "print(\"âœ… Training Complete.\")\n",
    "\n",
    "# 5. Evaluate (Silhouette Score)\n",
    "predictions_km = kmeans_model.transform(test_set)\n",
    "evaluator_km = ClusteringEvaluator()\n",
    "silhouette = evaluator_km.evaluate(predictions_km)\n",
    "print(f\"âœ… K-Means Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "# 6. Save Model\n",
    "model_path = f\"{MODELS_DIR}/spark_kmeans_model\"\n",
    "kmeans_model.save(model_path)\n",
    "print(f\"ðŸ’¾ Model saved to: {model_path}\")\n",
    "\n",
    "# 7. Clean up RAM\n",
    "train_set.unpersist()\n",
    "print(\"ðŸ§¹ RAM cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb275-0f37-4ef5-aeb7-eb153a282b3d",
   "metadata": {},
   "source": [
    "### Verify Saved Models (Dry Run Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d3981bc-bd4e-4a72-ad5e-1504d12f39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Model Loading ---\n",
      "âœ… Successfully loaded Logistic Regression model.\n",
      "\n",
      "ðŸ“Š Predictions:\n",
      "   1. \"Finland turns down US request for eggs\"\n",
      "      â†’ Predicted: r/worldnews\n",
      "\n",
      "   2. \"I don't want $5,000 from DOGE \"\n",
      "      â†’ Predicted: r/socialism\n",
      "\n",
      "   3. \"Today's Russia is the anti thesis of everything USSR once was.\"\n",
      "      â†’ Predicted: r/Conservative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Verifying Model Loading ---\")\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "try:\n",
    "    # 1. Load the model\n",
    "    loaded_lr = PipelineModel.load(f\"{MODELS_DIR}/spark_lr_model\")\n",
    "    print(\"âœ… Successfully loaded Logistic Regression model.\")\n",
    "    \n",
    "    # 2. Create sample data\n",
    "    sample_data = [\n",
    "        (\"Finland turns down US request for eggs\",),\n",
    "        (\"I don't want $5,000 from DOGE \",),\n",
    "        (\"Today's Russia is the anti thesis of everything USSR once was.\",)\n",
    "    ]\n",
    "    sample_df = spark.createDataFrame(sample_data, [\"title\"])\n",
    "    \n",
    "    # Add a dummy 'subreddit' column ---\n",
    "    # The pipeline expects this column to exist to pass the StringIndexer stage.\n",
    "    # We fill it with a dummy value (e.g., \"politics\" or null).\n",
    "    sample_df_ready = sample_df.withColumn(\"subreddit\", lit(\"politics\"))\n",
    "    \n",
    "    # 3. Run Prediction\n",
    "    result = loaded_lr.transform(sample_df_ready)\n",
    "    \n",
    "    # 4. Extract result\n",
    "    # prediction = result.select(\"predicted_subreddit\").first()[0]\n",
    "    predictions = result.select(\"title\", \"predicted_subreddit\").collect()\n",
    "    # print(f\"   Test Prediction for '{sample_data[0][0]}': r/{prediction}\")\n",
    "    print(\"\\nðŸ“Š Predictions:\")\n",
    "    for i, row in enumerate(predictions, 1):\n",
    "        print(f\"   {i}. \\\"{row['title']}\\\"\")\n",
    "        print(f\"      â†’ Predicted: r/{row['predicted_subreddit']}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load or run model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612fe8e-7cff-4968-afd4-4f70e8e8ec12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
