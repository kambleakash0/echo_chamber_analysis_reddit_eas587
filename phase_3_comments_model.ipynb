{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac28bae0-97f7-4687-b185-e532338d1649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "‚úÖ Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, LongType\n",
    "\n",
    "# Spark MLlib Imports\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session (Local Config)\n",
    "# We use local[*] to use all cores, and ensure enough memory for caching.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit_Comments_Model_Local\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = \"data_cleaned\"\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "print(\"‚úÖ Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6fcb0d-02c2-4e1c-9c67-90b4113c6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading comments with multiLine support...\n",
      "‚úÖ Data loaded and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Load all comments data\n",
    "comments_path = f\"{DATA_DIR}/*_comments.csv\"\n",
    "\n",
    "print(\"Loading comments with multiLine support...\")\n",
    "\n",
    "# 1. Read with multiLine enabled to handle newlines in comments\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(comments_path)\n",
    "\n",
    "# 2. Define Valid Subreddits (The Firewall)\n",
    "valid_subreddits = [\n",
    "    'Conservative', 'Libertarian', 'PoliticalDiscussion', \n",
    "    'neutralnews', 'politics', 'socialism', 'worldnews'\n",
    "]\n",
    "\n",
    "# 3. Strict Cleaning & Filtering\n",
    "# We filter out deleted comments, short comments, and rows where columns shifted.\n",
    "df_clean = df_raw.filter(\n",
    "    (F.col(\"body\").isNotNull()) & \n",
    "    (F.col(\"subreddit\").isin(valid_subreddits)) & # Discards garbage rows\n",
    "    (F.col(\"body\") != \"[deleted]\") & \n",
    "    (F.col(\"body\") != \"[removed]\") &\n",
    "    (F.length(F.col(\"body\")) > 15) # Keep only substantial comments (>15 chars)\n",
    ").select(\n",
    "    F.col(\"body\").alias(\"text\"), # Rename 'body' to 'text' for the pipeline\n",
    "    F.col(\"subreddit\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bee07b-b52b-4f87-8b64-f0bc2ae7c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts per subreddit:\n",
      "+-------------------+------+\n",
      "|          subreddit| count|\n",
      "+-------------------+------+\n",
      "|           politics|695286|\n",
      "|          worldnews|233914|\n",
      "|       Conservative| 44041|\n",
      "|          socialism| 14494|\n",
      "|        Libertarian| 16471|\n",
      "|PoliticalDiscussion| 10699|\n",
      "|        neutralnews|   686|\n",
      "+-------------------+------+\n",
      "\n",
      "Materializing balanced dataset into RAM...\n",
      "‚úÖ Final Training Set Size: 162,509 rows\n",
      "Balanced counts:\n",
      "+-------------------+-----+\n",
      "|          subreddit|count|\n",
      "+-------------------+-----+\n",
      "|       Conservative|39961|\n",
      "|        Libertarian|16471|\n",
      "|PoliticalDiscussion|10699|\n",
      "|        neutralnews|  686|\n",
      "|           politics|39935|\n",
      "|          socialism|14494|\n",
      "|          worldnews|40263|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate counts to see imbalance\n",
    "print(\"Original counts per subreddit:\")\n",
    "df_clean.groupBy(\"subreddit\").count().show()\n",
    "\n",
    "# 2. Define target sample size per class\n",
    "# 40k is a sweet spot: 40k * 7 = 280k rows. \n",
    "# This fits easily in RAM but is \"Big Data\" enough for a good model.\n",
    "TARGET_PER_CLASS = 40000\n",
    "\n",
    "# 3. Stratified Sampling\n",
    "# We loop through each subreddit and take a random sample.\n",
    "sampled_dfs = []\n",
    "for sub in valid_subreddits:\n",
    "    # Filter for specific subreddit\n",
    "    sub_df = df_clean.filter(F.col(\"subreddit\") == sub)\n",
    "    \n",
    "    # Calculate fraction needed to get TARGET_PER_CLASS\n",
    "    count = sub_df.count()\n",
    "    if count > TARGET_PER_CLASS:\n",
    "        fraction = TARGET_PER_CLASS / count\n",
    "        sampled_dfs.append(sub_df.sample(withReplacement=False, fraction=fraction, seed=42))\n",
    "    else:\n",
    "        # If less than target, take all of them\n",
    "        sampled_dfs.append(sub_df)\n",
    "\n",
    "# 4. Union all samples together\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "df_balanced = reduce(DataFrame.union, sampled_dfs)\n",
    "\n",
    "# 5. CACHE THE RESULT\n",
    "# This is the critical step. We load this clean, balanced dataset into RAM.\n",
    "df_balanced.cache()\n",
    "print(f\"Materializing balanced dataset into RAM...\")\n",
    "total_count = df_balanced.count()\n",
    "print(f\"‚úÖ Final Training Set Size: {total_count:,} rows\")\n",
    "\n",
    "print(\"Balanced counts:\")\n",
    "df_balanced.groupBy(\"subreddit\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e4e89d-f1ca-4c4b-b59d-7b8087e1d048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression on Comments ---\n",
      "Training model... (This might take 1-2 minutes)\n",
      "‚úÖ Training Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Logistic Regression on Comments ---\")\n",
    "\n",
    "# 1. Split Data\n",
    "train_data, test_data = df_balanced.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 2. Define Pipeline Stages\n",
    "# StringIndexer: Convert \"politics\" -> 0\n",
    "label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "\n",
    "# Tokenizer: Split text into words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# StopWords: Remove \"the\", \"and\", etc.\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# HashingTF: Convert words to numbers. \n",
    "# We use 10,000 features. Since we have 280k rows cached, this should fit in 4GB RAM.\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# IDF: Weight rare words higher\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Classifier\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# LabelConverter: Convert 0 -> \"politics\"\n",
    "# We need to fit the indexer first to get the labels for the converter\n",
    "indexer_model = label_indexer.fit(train_data)\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_subreddit\", labels=indexer_model.labels)\n",
    "\n",
    "# Build the Pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, hashingTF, idf, lr, label_converter])\n",
    "\n",
    "# 3. Train\n",
    "print(\"Training model... (This might take 1-2 minutes)\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"‚úÖ Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41393d3-7872-4e66-a82c-7059315c1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set...\n",
      "‚úÖ Comments Model Accuracy: 0.4343\n",
      "üíæ Model saved to: models/spark_lr_comments_model\n"
     ]
    }
   ],
   "source": [
    "# 1. Evaluate\n",
    "print(\"Evaluating on Test Set...\")\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"‚úÖ Comments Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 2. Save Model\n",
    "model_save_path = f\"{MODELS_DIR}/spark_lr_comments_model\"\n",
    "\n",
    "# Clean up old model if exists\n",
    "if os.path.exists(model_save_path):\n",
    "    shutil.rmtree(model_save_path)\n",
    "\n",
    "model.save(model_save_path)\n",
    "print(f\"üíæ Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95300a1f-8001-4a1e-bbe4-23a9228396ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Model Loading ---\n",
      "‚úÖ Successfully loaded Comments model.\n",
      "   Test Prediction: worldnews\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Verifying Model Loading ---\")\n",
    "\n",
    "try:\n",
    "    loaded_model = PipelineModel.load(model_save_path)\n",
    "    print(\"‚úÖ Successfully loaded Comments model.\")\n",
    "    \n",
    "    # Test a sample comment\n",
    "    # Note: The column name must be 'text' because that's what we trained on!\n",
    "    sample_text = [(\"I think the government spending is getting out of control.\",)]\n",
    "    sample_df = spark.createDataFrame(sample_text, [\"text\"])\n",
    "    \n",
    "    # Add dummy subreddit col for StringIndexer\n",
    "    sample_df = sample_df.withColumn(\"subreddit\", F.lit(\"politics\"))\n",
    "    \n",
    "    result = loaded_model.transform(sample_df)\n",
    "    prediction = result.select(\"predicted_subreddit\").first()[0]\n",
    "    print(f\"   Test Prediction: {prediction}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02568d19-8f53-42fb-8674-ff8078b467e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
