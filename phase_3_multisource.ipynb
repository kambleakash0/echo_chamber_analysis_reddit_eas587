{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be810c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, LongType, IntegerType, DoubleType, BooleanType, StructType, StructField, TimestampType\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress Java/Spark logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.spark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.hadoop\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "# Set JAVA_HOME to the Homebrew OpenJDK installation\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Java 17+ compatibility flags for Spark\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--conf spark.driver.extraJavaOptions='--add-opens java.base/javax.security.auth=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.util.calendar=ALL-UNNAMED ' \"\n",
    "    \"--conf spark.executor.extraJavaOptions='--add-opens java.base/javax.security.auth=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.util.calendar=ALL-UNNAMED ' pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4e9db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.17+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.17+0, mixed mode, sharing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Java\n",
    "os.system(\"java -version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02db03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing local Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/05 23:02:18 WARN Utils: Your hostname, Akashs-MacBook-Air-9.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.224 instead (on interface en0)\n",
      "25/11/05 23:02:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 23:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized. Running Spark version 4.0.1\n"
     ]
    }
   ],
   "source": [
    "TARGET_DATE = \"2025-09-10\"\n",
    "\n",
    "print(\"Initializing local Spark session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditEchoChamberLocal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark session initialized. Running Spark version {sc.version}\")\n",
    "\n",
    "# Also suppress Spark context logs\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbdac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and filtering Reddit data for 2025-09-10 ---\n",
      "Found 2,857 Reddit posts for 2025-09-10.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Loading and filtering Reddit data for {TARGET_DATE} ---\")\n",
    "data_dir = Path(\"./data_cleaned\")\n",
    "posts_path = str(data_dir / \"*_posts.csv\")\n",
    "posts_df_string = spark.read.csv(posts_path, header=True, inferSchema=False)\n",
    "posts_transformed_df = posts_df_string.withColumn(\n",
    "    \"timestamp\", F.to_timestamp(F.from_unixtime(F.expr(\"try_cast(created_utc AS LONG)\")))\n",
    ").withColumn(\n",
    "    \"score\", F.expr(\"try_cast(score AS INT)\")\n",
    ").withColumn(\n",
    "    \"num_comments\", F.expr(\"try_cast(num_comments AS INT)\")\n",
    ")\n",
    "reddit_with_date = posts_transformed_df.withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
    "reddit_single_day_df = reddit_with_date.filter(F.col(\"date\") == F.to_date(F.lit(TARGET_DATE)))\n",
    "reddit_single_day_df.cache()\n",
    "\n",
    "reddit_count = reddit_single_day_df.count()\n",
    "if reddit_count == 0:\n",
    "    print(f\"Warning: No Reddit posts found for {TARGET_DATE}. The analysis will be empty.\")\n",
    "else:\n",
    "    print(f\"Found {reddit_count:,} Reddit posts for {TARGET_DATE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6824afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Acquiring, loading, and correctly parsing GDELT data for 2025-09-10 ---\n",
      "GDELT data loaded, parsed, and sampled. Total events in sample: 2,857\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Acquiring, loading, and correctly parsing GDELT data for {TARGET_DATE} ---\")\n",
    "# (Download logic remains the same)\n",
    "gdelt_date_str = TARGET_DATE.replace(\"-\", \"\")\n",
    "gdelt_dir = Path(\"./gdelt_data\")\n",
    "gdelt_zip_filename = f\"{gdelt_date_str}.export.CSV.zip\"\n",
    "gdelt_csv_filename = f\"{gdelt_date_str}.export.CSV\"\n",
    "gdelt_zip_path = gdelt_dir / gdelt_zip_filename\n",
    "gdelt_csv_path = gdelt_dir / gdelt_csv_filename\n",
    "gdelt_url = f\"http://data.gdeltproject.org/events/{gdelt_zip_filename}\"\n",
    "os.makedirs(gdelt_dir, exist_ok=True)\n",
    "if not gdelt_csv_path.exists():\n",
    "    try:\n",
    "        urllib.request.urlretrieve(gdelt_url, gdelt_zip_path)\n",
    "        with zipfile.ZipFile(gdelt_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(gdelt_dir)\n",
    "        os.remove(gdelt_zip_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to download GDELT data: {e}\")\n",
    "\n",
    "gdelt_schema = StructType([\n",
    "    StructField(\"GLOBALEVENTID\", StringType(), True), StructField(\"SQLDATE\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True), StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True), StructField(\"GoldsteinScale\", DoubleType(), True),\n",
    "    StructField(\"NumMentions\", StringType(), True), StructField(\"NumSources\", StringType(), True),\n",
    "    StructField(\"NumArticles\", StringType(), True), StructField(\"AvgTone\", DoubleType(), True),\n",
    "    StructField(\"SOURCEURL\", StringType(), True)\n",
    "])\n",
    "\n",
    "gdelt_sample_df = None\n",
    "if gdelt_csv_path.exists() and reddit_count > 0:\n",
    "    # --- THE KEY FIX: Read without a schema and select by index ---\n",
    "    gdelt_raw_df = spark.read.csv(str(gdelt_csv_path), sep='\\\\t', inferSchema=False)\n",
    "    \n",
    "    # GDELT column indices (0-based) from the official documentation\n",
    "    # SQLDATE=1, Actor1Name=5, EventCode=26, GoldsteinScale=30, AvgTone=34\n",
    "    gdelt_df = gdelt_raw_df.select(\n",
    "        F.col(\"_c1\").alias(\"SQLDATE\"),\n",
    "        F.col(\"_c5\").alias(\"Actor1Name\"),\n",
    "        F.col(\"_c26\").alias(\"EventCode\"),\n",
    "        F.col(\"_c30\").cast(\"double\").alias(\"GoldsteinScale\"),\n",
    "        F.col(\"_c34\").cast(\"double\").alias(\"AvgTone\")\n",
    "    )\n",
    "    \n",
    "    gdelt_sample_df = gdelt_df.limit(reddit_count)\n",
    "    gdelt_sample_df.cache()\n",
    "    print(f\"GDELT data loaded, parsed, and sampled. Total events in sample: {gdelt_sample_df.count():,}\")\n",
    "else:\n",
    "    print(\"GDELT data not loaded or no Reddit data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204d54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Deriving Comparative Insights for 2025-09-10 ---\n",
      "\n",
      "Insight 1: Overall Sentiment Comparison (News vs. Reddit)\n",
      "  - Average GDELT News Sentiment (AvgTone): -2.83\n",
      "  - Average Reddit Post Score: 253.32\n",
      "\n",
      "Insight 2: Mentions of Top GDELT Actors in Reddit Titles\n",
      "Top 5 Filtered GDELT Actors in sample: ['USA', 'ISR', 'QAT', 'BUS', 'GBR']\n",
      "Mentions of these actors in Reddit titles:\n",
      "  - 'USA': 33 mentions\n",
      "  - 'ISR': 53 mentions\n",
      "  - 'QAT': 17 mentions\n",
      "  - 'BUS': 7 mentions\n",
      "  - 'GBR': 0 mentions\n",
      "\n",
      "Insight 3: Thematic Focus Comparison (GDELT Events vs. Reddit Keywords)\n",
      "  - Top 10 GDELT Event Types:\n",
      "+---------+-----+\n",
      "|EventCode|count|\n",
      "+---------+-----+\n",
      "|010      |254  |\n",
      "|042      |224  |\n",
      "|020      |201  |\n",
      "|043      |198  |\n",
      "|051      |168  |\n",
      "|040      |154  |\n",
      "|190      |153  |\n",
      "|111      |148  |\n",
      "|173      |79   |\n",
      "|046      |78   |\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "  - Top 10 Reddit Title Keywords (after robust stop word removal):\n",
      "+--------+-----+\n",
      "|word    |count|\n",
      "+--------+-----+\n",
      "|charlie |1219 |\n",
      "|kirk    |1163 |\n",
      "|shot    |517  |\n",
      "|utah    |439  |\n",
      "|trump   |432  |\n",
      "|event   |312  |\n",
      "|dead    |201  |\n",
      "|says    |172  |\n",
      "|removed |157  |\n",
      "|activist|154  |\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "if gdelt_sample_df and reddit_single_day_df.count() > 0:\n",
    "    print(f\"\\n--- Deriving Comparative Insights for {TARGET_DATE} ---\")\n",
    "    \n",
    "    # Insight 1: (No changes needed, the result is already insightful)\n",
    "    print(\"\\nInsight 1: Overall Sentiment Comparison (News vs. Reddit)\")\n",
    "    gdelt_avg_sentiment = gdelt_sample_df.agg(F.avg(\"AvgTone\").alias(\"avg_news_sentiment\")).first()['avg_news_sentiment']\n",
    "    reddit_avg_score = reddit_single_day_df.agg(F.avg(\"score\").alias(\"avg_reddit_score\")).first()['avg_reddit_score']\n",
    "    print(f\"  - Average GDELT News Sentiment (AvgTone): {gdelt_avg_sentiment:.2f}\")\n",
    "    print(f\"  - Average Reddit Post Score: {reddit_avg_score:.2f}\")\n",
    "\n",
    "    # Insight 2: Find Mentions of Top GDELT Actors in Reddit Titles (with case-insensitivity)\n",
    "    print(f\"\\nInsight 2: Mentions of Top GDELT Actors in Reddit Titles\")\n",
    "    generic_actors = [\"PROTESTER\", \"GOVERNMENT\", \"POLICE\", \"REBEL\", \"MILITARY\", \"CITIZEN\", \"MEDIA\", \"OPPOSITION\", \"GOV\", \"JUD\"]\n",
    "    top_actors = gdelt_sample_df.filter(\n",
    "        F.col(\"Actor1Name\").isNotNull() & (~F.col(\"Actor1Name\").isin(generic_actors))\n",
    "    ).groupBy(\"Actor1Name\").count().orderBy(F.col(\"count\").desc()).limit(5)\n",
    "    top_actors_list = [row.Actor1Name for row in top_actors.collect()]\n",
    "    \n",
    "    if not top_actors_list:\n",
    "        print(\"No significant non-generic actors found in the GDELT sample.\")\n",
    "    else:\n",
    "        print(f\"Top 5 Filtered GDELT Actors in sample: {top_actors_list}\")\n",
    "        mention_counts = []\n",
    "        for actor in top_actors_list:\n",
    "            # THE FIX: Use F.lower() for a case-insensitive search\n",
    "            count = reddit_single_day_df.filter(F.locate(actor.lower(), F.lower(F.col(\"title\"))) > 0).count()\n",
    "            mention_counts.append((actor, count))\n",
    "        print(\"Mentions of these actors in Reddit titles:\")\n",
    "        for actor, count in mention_counts:\n",
    "            print(f\"  - '{actor}': {count} mentions\")\n",
    "\n",
    "    # Insight 3: Compare Thematic Focus (with robust stop word removal)\n",
    "    print(f\"\\nInsight 3: Thematic Focus Comparison (GDELT Events vs. Reddit Keywords)\")\n",
    "    print(\"  - Top 10 GDELT Event Types:\")\n",
    "    gdelt_sample_df.groupBy(\"EventCode\").count().orderBy(F.col(\"count\").desc()).show(10, truncate=False)\n",
    "    \n",
    "    # THE FIX: Use a standard list of stop words for a more reliable filter\n",
    "    print(\"  - Top 10 Reddit Title Keywords (after robust stop word removal):\")\n",
    "    # Get the default stop words from the ML library\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    \n",
    "    reddit_keywords = reddit_single_day_df.withColumn(\"word\", F.explode(F.split(F.lower(F.col(\"title\")), \"[^a-zA-Z]\"))) \\\n",
    "        .filter(~F.col(\"word\").isin(stopwords)) \\\n",
    "        .filter(F.col(\"word\") != \"\") \\\n",
    "        .groupBy(\"word\").count().orderBy(F.col(\"count\").desc())\n",
    "    reddit_keywords.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9059ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Spark session stopped. Script finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 22:58:54 ERROR Executor: Exception in task 7.0 in stage 12.0 (TID 43): Block rdd_47_7 does not exist\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"\\n✅ Spark session stopped. Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
