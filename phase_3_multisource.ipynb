{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be810c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, LongType, IntegerType, DoubleType, BooleanType, StructType, StructField, TimestampType\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress Java/Spark logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.spark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.hadoop\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4e9db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Java\n",
    "os.system(\"java -version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02db03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "TARGET_DATE = \"2025-09-09\"\n",
    "data_dir = Path(\"./data_cleaned\")\n",
    "posts_path = str(data_dir / \"*_posts.csv\")\n",
    "comments_path = str(data_dir / \"*_comments.csv\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccbdac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and filtering Reddit data for 2025-09-09 ---\n",
      "Found 1,255 Reddit posts for 2025-09-09.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Loading and filtering Reddit data for {TARGET_DATE} ---\")\n",
    "posts_path = str(data_dir / \"*_posts.csv\")\n",
    "posts_df_string = spark.read.csv(posts_path, header=True, inferSchema=False)\n",
    "posts_transformed_df = posts_df_string.withColumn(\n",
    "    \"timestamp\", F.to_timestamp(F.from_unixtime(F.expr(\"try_cast(created_utc AS LONG)\")))\n",
    ").withColumn(\n",
    "    \"score\", F.expr(\"try_cast(score AS INT)\")\n",
    ").withColumn(\n",
    "    \"num_comments\", F.expr(\"try_cast(num_comments AS INT)\")\n",
    ")\n",
    "reddit_with_date = posts_transformed_df.withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
    "reddit_single_day_df = reddit_with_date.filter(F.col(\"date\") == F.to_date(F.lit(TARGET_DATE)))\n",
    "# reddit_single_day_df.cache()\n",
    "\n",
    "reddit_count = reddit_single_day_df.count()\n",
    "if reddit_count == 0:\n",
    "    print(f\"Warning: No Reddit posts found for {TARGET_DATE}. The analysis will be empty.\")\n",
    "else:\n",
    "    print(f\"Found {reddit_count:,} Reddit posts for {TARGET_DATE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6824afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Acquiring GDELT data (Sept 01 - Sept 25) ---\n",
      "\n",
      "Batch download complete. 0 new files acquired.\n",
      "Loading GDELT data into Spark...\n",
      "âœ… Successfully loaded 25 days of GDELT data.\n",
      "ðŸ“Š Total Global Events in DataFrame: 2,993,530\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Batch Acquiring GDELT data (Sept 01 - Sept 25) ---\")\n",
    "\n",
    "gdelt_dir = Path(\"./gdelt_data\")\n",
    "os.makedirs(gdelt_dir, exist_ok=True)\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime.date(2025, 9, 1)\n",
    "end_date = datetime.date(2025, 9, 25)\n",
    "delta = datetime.timedelta(days=1)\n",
    "\n",
    "current_date = start_date\n",
    "files_downloaded = 0\n",
    "\n",
    "while current_date <= end_date:\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")\n",
    "    gdelt_zip_filename = f\"{date_str}.export.CSV.zip\"\n",
    "    gdelt_csv_filename = f\"{date_str}.export.CSV\"\n",
    "    gdelt_zip_path = gdelt_dir / gdelt_zip_filename\n",
    "    gdelt_csv_path = gdelt_dir / gdelt_csv_filename\n",
    "    gdelt_url = f\"http://data.gdeltproject.org/events/{gdelt_zip_filename}\"\n",
    "\n",
    "    # Check if CSV already exists\n",
    "    if not gdelt_csv_path.exists():\n",
    "        print(f\"[{date_str}] Downloading...\", end=\" \", flush=True)\n",
    "        try:\n",
    "            # Download\n",
    "            urllib.request.urlretrieve(gdelt_url, gdelt_zip_path)\n",
    "            \n",
    "            # Unzip\n",
    "            with zipfile.ZipFile(gdelt_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(gdelt_dir)\n",
    "            \n",
    "            # Cleanup Zip\n",
    "            os.remove(gdelt_zip_path)\n",
    "            print(\"âœ… Done.\")\n",
    "            files_downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed: {e}\")\n",
    "    else:\n",
    "        # print(f\"[{date_str}] Already exists.\") # Uncomment to see skipped files\n",
    "        pass\n",
    "\n",
    "    current_date += delta\n",
    "\n",
    "print(f\"\\nBatch download complete. {files_downloaded} new files acquired.\")\n",
    "\n",
    "# --- LOAD ALL FILES INTO SPARK ---\n",
    "# Spark can read a directory or a list of files as a single DataFrame!\n",
    "print(\"Loading GDELT data into Spark...\")\n",
    "\n",
    "# Get list of all CSVs in the directory\n",
    "all_gdelt_files = [str(p) for p in gdelt_dir.glob(\"*.export.CSV\")]\n",
    "\n",
    "if all_gdelt_files:\n",
    "    # Read without schema first (Robust Method)\n",
    "    # Spark automatically unions all these files into one big DataFrame\n",
    "    gdelt_raw_df = spark.read.csv(all_gdelt_files, sep='\\t', inferSchema=False)\n",
    "    \n",
    "    # Select columns by index (Standard GDELT Layout)\n",
    "    # SQLDATE=1, Actor1Name=5, EventCode=26, GoldsteinScale=30, AvgTone=34\n",
    "    gdelt_df = gdelt_raw_df.select(\n",
    "        F.col(\"_c1\").alias(\"SQLDATE\"),\n",
    "        F.col(\"_c5\").alias(\"Actor1Name\"),\n",
    "        F.col(\"_c26\").alias(\"EventCode\"),\n",
    "        F.col(\"_c30\").cast(\"double\").alias(\"GoldsteinScale\"),\n",
    "        F.col(\"_c34\").cast(\"double\").alias(\"AvgTone\")\n",
    "    ).withColumn(\"date\", F.to_date(F.col(\"SQLDATE\"), \"yyyyMMdd\"))\n",
    "    \n",
    "    # Cache this larger dataset\n",
    "    gdelt_df.cache()\n",
    "    total_events = gdelt_df.count()\n",
    "    print(f\"âœ… Successfully loaded {len(all_gdelt_files)} days of GDELT data.\")\n",
    "    print(f\"ðŸ“Š Total Global Events in DataFrame: {total_events:,}\")\n",
    "else:\n",
    "    print(\"âŒ No GDELT files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204d54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Deriving Comparative Insights for 2025-09-09 ---\n",
      "\n",
      "Insight 1: Overall Sentiment Comparison (News vs. Reddit)\n",
      "  - Average GDELT News Sentiment (AvgTone): -1.99\n",
      "  - Average Reddit Post Score: 451.43\n",
      "\n",
      "Insight 2: Mentions of Top GDELT Actors in Reddit Titles\n",
      "Top 5 Filtered GDELT Actors in sample: ['USA', 'GBR', 'COP', 'BUS', 'EDU']\n",
      "Mentions of these actors in Reddit titles:\n",
      "  - 'USA': 14 mentions\n",
      "  - 'GBR': 0 mentions\n",
      "  - 'COP': 5 mentions\n",
      "  - 'BUS': 7 mentions\n",
      "  - 'EDU': 6 mentions\n",
      "\n",
      "Insight 3: Thematic Focus Comparison (GDELT Events vs. Reddit Keywords)\n",
      "  - Top 10 GDELT Event Types:\n",
      "+---------+------+\n",
      "|EventCode|count |\n",
      "+---------+------+\n",
      "|010      |240046|\n",
      "|042      |233748|\n",
      "|043      |218977|\n",
      "|051      |195411|\n",
      "|040      |191740|\n",
      "|020      |173823|\n",
      "|190      |115235|\n",
      "|036      |114339|\n",
      "|046      |95323 |\n",
      "|173      |94551 |\n",
      "+---------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "  - Top 10 Reddit Title Keywords (after robust stop word removal):\n",
      "+--------+-----+\n",
      "|word    |count|\n",
      "+--------+-----+\n",
      "|trump   |270  |\n",
      "|epstein |158  |\n",
      "|birthday|94   |\n",
      "|israel  |88   |\n",
      "|hamas   |86   |\n",
      "|qatar   |77   |\n",
      "|us      |70   |\n",
      "|nepal   |68   |\n",
      "|says    |68   |\n",
      "|letter  |58   |\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if gdelt_df and reddit_single_day_df.count() > 0:\n",
    "    print(f\"\\n--- Deriving Comparative Insights for {TARGET_DATE} ---\")\n",
    "    \n",
    "    # Insight 1: (No changes needed, the result is already insightful)\n",
    "    print(\"\\nInsight 1: Overall Sentiment Comparison (News vs. Reddit)\")\n",
    "    gdelt_avg_sentiment = gdelt_df.agg(F.avg(\"AvgTone\").alias(\"avg_news_sentiment\")).first()['avg_news_sentiment']\n",
    "    reddit_avg_score = reddit_single_day_df.agg(F.avg(\"score\").alias(\"avg_reddit_score\")).first()['avg_reddit_score']\n",
    "    print(f\"  - Average GDELT News Sentiment (AvgTone): {gdelt_avg_sentiment:.2f}\")\n",
    "    print(f\"  - Average Reddit Post Score: {reddit_avg_score:.2f}\")\n",
    "\n",
    "    # Insight 2: Find Mentions of Top GDELT Actors in Reddit Titles (with case-insensitivity)\n",
    "    print(f\"\\nInsight 2: Mentions of Top GDELT Actors in Reddit Titles\")\n",
    "    generic_actors = [\"PROTESTER\", \"GOVERNMENT\", \"POLICE\", \"REBEL\", \"MILITARY\", \"CITIZEN\", \"MEDIA\", \"OPPOSITION\", \"GOV\", \"JUD\"]\n",
    "    top_actors = gdelt_df.filter(\n",
    "        F.col(\"Actor1Name\").isNotNull() & (~F.col(\"Actor1Name\").isin(generic_actors))\n",
    "    ).groupBy(\"Actor1Name\").count().orderBy(F.col(\"count\").desc()).limit(5)\n",
    "    top_actors_list = [row.Actor1Name for row in top_actors.collect()]\n",
    "    \n",
    "    if not top_actors_list:\n",
    "        print(\"No significant non-generic actors found in the GDELT sample.\")\n",
    "    else:\n",
    "        print(f\"Top 5 Filtered GDELT Actors in sample: {top_actors_list}\")\n",
    "        mention_counts = []\n",
    "        for actor in top_actors_list:\n",
    "            # THE FIX: Use F.lower() for a case-insensitive search\n",
    "            count = reddit_single_day_df.filter(F.locate(actor.lower(), F.lower(F.col(\"title\"))) > 0).count()\n",
    "            mention_counts.append((actor, count))\n",
    "        print(\"Mentions of these actors in Reddit titles:\")\n",
    "        for actor, count in mention_counts:\n",
    "            print(f\"  - '{actor}': {count} mentions\")\n",
    "\n",
    "    # Insight 3: Compare Thematic Focus (with robust stop word removal)\n",
    "    print(f\"\\nInsight 3: Thematic Focus Comparison (GDELT Events vs. Reddit Keywords)\")\n",
    "    print(\"  - Top 10 GDELT Event Types:\")\n",
    "    gdelt_df.groupBy(\"EventCode\").count().orderBy(F.col(\"count\").desc()).show(10, truncate=False)\n",
    "    \n",
    "    # THE FIX: Use a standard list of stop words for a more reliable filter\n",
    "    print(\"  - Top 10 Reddit Title Keywords (after robust stop word removal):\")\n",
    "    # Get the default stop words from the ML library\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "    stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "    \n",
    "    reddit_keywords = reddit_single_day_df.withColumn(\"word\", F.explode(F.split(F.lower(F.col(\"title\")), \"[^a-zA-Z]\"))) \\\n",
    "        .filter(~F.col(\"word\").isin(stopwords)) \\\n",
    "        .filter(F.col(\"word\") != \"\") \\\n",
    "        .groupBy(\"word\").count().orderBy(F.col(\"count\").desc())\n",
    "    reddit_keywords.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9059ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Spark session stopped. Script finished.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"\\nâœ… Spark session stopped. Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4371de-ec1d-4abc-9b03-26b3c1757ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
