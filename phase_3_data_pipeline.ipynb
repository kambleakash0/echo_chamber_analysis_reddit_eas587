{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30e55448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, LongType, IntegerType, DoubleType, BooleanType\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress Java/Spark logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.spark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.apache.hadoop\").setLevel(logging.ERROR)\n",
    "\n",
    "# Also suppress Spark context logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Set JAVA_HOME to the Homebrew OpenJDK installation\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Java 17+ compatibility flags for Spark\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--conf spark.driver.extraJavaOptions='--add-opens java.base/javax.security.auth=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.util.calendar=ALL-UNNAMED ' \"\n",
    "    \"--conf spark.executor.extraJavaOptions='--add-opens java.base/javax.security.auth=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.invoke=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.lang.reflect=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.io=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.net=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.nio=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/java.util.concurrent.atomic=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.ch=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.nio.cs=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.security.action=ALL-UNNAMED \"\n",
    "    \"--add-opens java.base/sun.util.calendar=ALL-UNNAMED ' pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bfeff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.17+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.17+0, mixed mode, sharing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Java\n",
    "os.system(\"java -version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07864914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing local Spark session...\n",
      "Spark session initialized. Running Spark version 4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing local Spark session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditEchoChamberLocal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark session initialized. Running Spark version {sc.version}\")\n",
    "\n",
    "# Also suppress Spark context logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "435ca921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17.0.17'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jvm.java.lang.System.getProperty(\"java.version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6be1588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading posts from: data_cleaned/*_posts.csv\n",
      "Loading comments from: data_cleaned/*_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Posts DataFrame Schema and Count:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- is_self: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- num_comments: string (nullable = true)\n",
      " |-- upvote_ratio: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- stickied: string (nullable = true)\n",
      " |-- over_18: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      "\n",
      "Total posts: 35,551\n",
      "\n",
      "Comments DataFrame Schema and Count:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- controversiality: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- is_submitter: string (nullable = true)\n",
      "\n",
      "Total comments: 1,831,209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the local paths to your data\n",
    "# The script assumes your 'data_cleaned' folder is in the same directory\n",
    "data_dir = Path(\"./data_cleaned\")\n",
    "posts_path = str(data_dir / \"*_posts.csv\")\n",
    "comments_path = str(data_dir / \"*_comments.csv\")\n",
    "\n",
    "print(f\"\\nLoading posts from: {posts_path}\")\n",
    "print(f\"Loading comments from: {comments_path}\")\n",
    "\n",
    "# Load the posts and comments into Spark DataFrames\n",
    "# The wildcard (*) tells Spark to load all matching files into one DataFrame\n",
    "try:\n",
    "    posts_df = spark.read.csv(posts_path, header=True, inferSchema=True)\n",
    "    comments_df = spark.read.csv(comments_path, header=True, inferSchema=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Cache the DataFrames in memory for faster access\n",
    "posts_df.cache()\n",
    "comments_df.cache()\n",
    "\n",
    "# Verify the data is loaded correctly\n",
    "print(\"\\nPosts DataFrame Schema and Count:\")\n",
    "posts_df.printSchema()\n",
    "print(f\"Total posts: {posts_df.count():,}\")\n",
    "\n",
    "print(\"\\nComments DataFrame Schema and Count:\")\n",
    "comments_df.printSchema()\n",
    "print(f\"Total comments: {comments_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19adcc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5+ Spark Transformations (Production-Ready Method) ---\n",
      "\n",
      "Found posts by author 'hety0p':\n",
      "+---------+------+-----------------------------------------------------------------------------------------------+-----+------------+-------------------+\n",
      "|subreddit|author|title                                                                                          |score|num_comments|timestamp          |\n",
      "+---------+------+-----------------------------------------------------------------------------------------------+-----+------------+-------------------+\n",
      "|politics |hety0p|Rubio will meet Mexico's president as Trump flexes military might in Latin America             |4    |3           |2025-09-03 11:52:02|\n",
      "|politics |hety0p|Nancy Mace Fires Back at Claim She Hijacked Epstein Survivors’ Meeting                         |1    |0           |2025-09-03 12:15:52|\n",
      "|politics |hety0p|If Trump loses his tariff lawsuit, America may have to refund businesses more than $200 billion|1    |0           |2025-09-03 12:19:05|\n",
      "|politics |hety0p|Florida will work to eliminate all childhood vaccine mandates in the state, officials say      |9    |15          |2025-09-03 12:58:20|\n",
      "|politics |hety0p|Trump’s Kennedy Center hosted a right-wing pep rally for Gen Z evangelicals                    |1    |0           |2025-09-03 13:12:44|\n",
      "+---------+------+-----------------------------------------------------------------------------------------------+-----+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Subreddit stats, ordered by average score:\n",
      "+--------------------+------------------+--------------+\n",
      "|           subreddit|         avg_score|total_comments|\n",
      "+--------------------+------------------+--------------+\n",
      "|           worldnews|  747.183061645119|        275135|\n",
      "| the president of...|             594.0|          NULL|\n",
      "|            politics| 582.3323339119457|        704614|\n",
      "| I think we shoul...|             504.0|            52|\n",
      "| this is the larg...|             373.0|          NULL|\n",
      "| but as a catalyt...|             373.0|          NULL|\n",
      "| I completely res...|             282.0|             3|\n",
      "| 'Pass The Torch'...|             226.0|          NULL|\n",
      "| and usually I ge...|             174.0|          NULL|\n",
      "| but there’s a di...|             164.0|          NULL|\n",
      "| some are our family|             163.0|          NULL|\n",
      "|         Libertarian| 150.5219512195122|         10875|\n",
      "|           socialism|148.29777777777778|         10382|\n",
      "| Lara Logan compa...|             142.0|          NULL|\n",
      "| I guess I’m just...|             134.0|          NULL|\n",
      "| but they've lost...|             122.0|          NULL|\n",
      "| but ignoring cri...|              91.0|            37|\n",
      "| silence. Where’s...|              79.0|          NULL|\n",
      "| I am not a Conse...|              77.0|            78|\n",
      "|        Conservative| 76.73289080029917|        155274|\n",
      "+--------------------+------------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Performing 5+ Spark Transformations (Production-Ready Method) ---\")\n",
    "\n",
    "# --- Data Loading with NO Schema Inference ---\n",
    "# Read all columns as strings to prevent any schema errors.\n",
    "posts_df_string = spark.read.csv(posts_path, header=True, inferSchema=False)\n",
    "\n",
    "# --- Explicit and Safe Casting using try_cast ---\n",
    "# This is the most robust way to handle dirty data.\n",
    "# We use F.expr() to invoke the powerful try_cast SQL function.\n",
    "posts_transformed_df = posts_df_string.withColumn(\n",
    "    # Safely cast created_utc to a number, then convert to timestamp\n",
    "    \"timestamp\", F.to_timestamp(F.from_unixtime(F.expr(\"try_cast(created_utc AS LONG)\")))\n",
    ").withColumn(\n",
    "    \"score\", F.expr(\"try_cast(score AS INT)\")\n",
    ").withColumn(\n",
    "    \"num_comments\", F.expr(\"try_cast(num_comments AS INT)\")\n",
    ")\n",
    "\n",
    "\n",
    "# 2. select: Create a more focused DataFrame\n",
    "focused_posts_df = posts_transformed_df.select(\"subreddit\", \"author\", \"title\", \"score\", \"num_comments\", \"timestamp\")\n",
    "\n",
    "# 3. filter: Find all posts by a specific, known active author\n",
    "print(\"\\nFound posts by author 'hety0p':\")\n",
    "author_posts_df = focused_posts_df.filter(F.col(\"author\") == \"hety0p\")\n",
    "author_posts_df.show(5, truncate=False)\n",
    "\n",
    "# 4. groupBy & agg: Find the average score and total comments per subreddit\n",
    "# The agg functions (avg, sum) will automatically ignore the NULLs created by try_cast.\n",
    "subreddit_stats_df = focused_posts_df.groupBy(\"subreddit\").agg(\n",
    "    F.avg(\"score\").alias(\"avg_score\"),\n",
    "    F.sum(\"num_comments\").alias(\"total_comments\")\n",
    ")\n",
    "\n",
    "# 5. orderBy: Show the subreddits with the highest average score\n",
    "print(\"\\nSubreddit stats, ordered by average score:\")\n",
    "subreddit_stats_df.orderBy(F.col(\"avg_score\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb6416c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance Comparison: Average Score per Subreddit ---\n",
      "Running with PySpark...\n",
      "PySpark execution time: 0.3318 seconds\n",
      "\n",
      "Running with Pandas...\n",
      "Pandas execution time: 0.2027 seconds\n",
      "\n",
      "Note: For small datasets (< several GBs), Pandas is often faster due to Spark's overhead.\n",
      "The power of Spark becomes evident at big data scales.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Performance Comparison: Average Score per Subreddit ---\")\n",
    "\n",
    "# --- Spark Performance ---\n",
    "print(\"Running with PySpark...\")\n",
    "start_time_spark = time.time()\n",
    "\n",
    "# THE FIX: Use the cleaned 'posts_transformed_df' instead of the raw 'posts_df'\n",
    "# The 'score' column in this DataFrame has already been safely cast.\n",
    "posts_transformed_df.groupBy(\"subreddit\").agg(F.avg(\"score\")).collect()\n",
    "\n",
    "end_time_spark = time.time()\n",
    "print(f\"PySpark execution time: {end_time_spark - start_time_spark:.4f} seconds\")\n",
    "\n",
    "# --- Pandas Performance ---\n",
    "print(\"\\nRunning with Pandas...\")\n",
    "start_time_pandas = time.time()\n",
    "# For a fair comparison, load all post CSVs into a single Pandas DataFrame\n",
    "all_post_files = list(data_dir.glob(\"*_posts.csv\"))\n",
    "pd_df_list = [pd.read_csv(f) for f in all_post_files]\n",
    "pd_posts_df = pd.concat(pd_df_list, ignore_index=True)\n",
    "\n",
    "# Add a similar safe casting for Pandas to avoid errors and ensure a fair comparison\n",
    "pd_posts_df['score'] = pd.to_numeric(pd_posts_df['score'], errors='coerce')\n",
    "pd_posts_df.groupby(\"subreddit\")[\"score\"].mean()\n",
    "\n",
    "end_time_pandas = time.time()\n",
    "print(f\"Pandas execution time: {end_time_pandas - start_time_pandas:.4f} seconds\")\n",
    "\n",
    "print(\"\\nNote: For small datasets (< several GBs), Pandas is often faster due to Spark's overhead.\")\n",
    "print(\"The power of Spark becomes evident at big data scales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2bba8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Spark session stopped. Script finished.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"\\n✅ Spark session stopped. Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
